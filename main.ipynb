{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn.common_types import _size_2_t\n",
    "from torch.nn.modules.utils import _pair\n",
    "from collections import OrderedDict, abc\n",
    "from itertools import repeat\n",
    "\n",
    "from typing import Type, Callable, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and learning final project\n",
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of memory,\n",
    "#try to reimplement according to https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/locally_connected/locally_connected2d.py#L34\n",
    "class LocalLinear_custom(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int,padding : int, image_size: int):\n",
    "        super(LocalLinear_custom, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        fold_num = (image_size+ 2* padding - self.kernel_size)//self.stride+1\n",
    "        self.weight = nn.Parameter(torch.randn( in_channels, fold_num,fold_num, kernel_size, kernel_size, out_channels))\n",
    "        self.bias = nn.Parameter(torch.randn(fold_num,fold_num,out_channels))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        #print(\"intial xshape\",x.shape)\n",
    "        #print(\"weight\",self.weight.shape)\n",
    "        #print(\"bias\",self.bias.shape)\n",
    "        x = F.pad(x, [self.padding]*4, value= 0)\n",
    "        #print(\"pad\",x.shape)\n",
    "        x = x.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        #print(\"unfold\",x.shape)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.matmul(x,self.weight).squeeze(2).sum([-1,-2,-5])\n",
    "        #print(\"matmul\",x.shape)\n",
    "        x = x + self.bias\n",
    "        #print(\"final xshape\",x.shape,end=\"\\n\\n\")\n",
    "        return x\n",
    "\n",
    "class view_custom(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funConv2d(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,int,List[nn.Module]]:\n",
    "    padding = kernel_size//2\n",
    "    \n",
    "    modules = [ \n",
    "    nn.Conv2d(size_in,size_out,kernel_size,stride = stride,padding = padding),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()]\n",
    "    im_out = (image_size+ 2 * padding - kernel_size)//stride+1\n",
    "    return im_out,size_out,modules\n",
    "\n",
    "def funLocalLinear(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,int,List[nn.Module]]:\n",
    "    padding = kernel_size//2\n",
    "    modules = [ \n",
    "    LocalLinear_custom(size_in,size_out,kernel_size,stride = stride,padding = padding,image_size = image_size),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()]\n",
    "    im_out = (image_size+ 2* padding - kernel_size)//stride + 1\n",
    "    return im_out,size_out,modules\n",
    "\n",
    "def funFullyConnected(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,int,List[nn.Module]]:\n",
    "    modules = []\n",
    "    modules.extend([\n",
    "    view_custom([-1,image_size*image_size*size_in]),\n",
    "    nn.Linear(size_in * image_size**2,size_out * (image_size//stride)**2),\n",
    "    view_custom([-1,size_out,image_size//stride,image_size//stride]),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()])\n",
    "    im_out = image_size//stride\n",
    "    return im_out,size_out,modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Conv(nn.Module):\n",
    "    ### only works with square image, kernel size and stride\n",
    "    def __init__(self, base_channel_size: int, loc : Callable[[int,int,int,int,int],tuple[int,int,List[nn.Module]]], image_size :int ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        alpha = base_channel_size\n",
    "        channel_size :int = 3\n",
    "        im_size : int = image_size\n",
    "        \n",
    "        modules : List[nn.Module] = []\n",
    "        size_conv = [\n",
    "            (alpha,1),\n",
    "            (2*alpha,2), #remove stride for now\n",
    "            (2*alpha,1),\n",
    "            (4*alpha,2), #here too\n",
    "            (4*alpha,1),\n",
    "            (8*alpha,2), # here too\n",
    "            (8*alpha,1),\n",
    "            (16*alpha,2)] # here too\n",
    "        \n",
    "        for i,val in enumerate(size_conv):\n",
    "            size_out, stride = val\n",
    "            im_size,channel_size,new_module = loc(channel_size,size_out,3,stride,im_size)\n",
    "            modules.extend(new_module)\n",
    "            \n",
    "        self.conv = nn.Sequential(*modules)\n",
    "        im_size,channel_size,modules = funFullyConnected(channel_size,64*alpha,0,1,im_size)\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        self.fc_final = nn.Linear(channel_size*im_size**2,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_final(x)\n",
    "        return x\n",
    "\n",
    "class S_Conv(nn.Module):\n",
    "    def __init__(self, base_channel_size: int, loc : Callable[[int,int,int,int,int],tuple[int,int,List[nn.Module]]], image_size :int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        alpha = base_channel_size\n",
    "        channel_size :int = 3\n",
    "        im_size : int = image_size\n",
    "        \n",
    "        im_size,channel_size, modules = loc(channel_size,alpha,9,2,im_size)\n",
    "        self.conv =  nn.Sequential(*modules)\n",
    "        \n",
    "        im_size,channel_size, modules = funFullyConnected(channel_size,24*alpha,0,1,im_size)\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc_final = nn.Linear(channel_size*im_size**2,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 512\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks = [(D_Conv,\"D_Conv\"),(S_Conv,\"S_Conv\")]\n",
    "convolutions = [(funConv2d,\"Conv2d\"),(funFullyConnected,\"FullyConnected\")] #(funLocalLinear,\"LocalLinear\"),\n",
    "nets = [(ni+\"_\"+nj,i,j) for i,ni in networks[1:] for j,nj in convolutions[1:] ]\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_Conv_FullyConnected\n",
      "************************* 25\n",
      "************************* 50\n",
      "************************* 75\n"
     ]
    }
   ],
   "source": [
    "for name,net_type,call in nets:\n",
    "    net = net_type(10,call,32).to(DEVICE)\n",
    "    print(name)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(400):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print('*', end='')\n",
    "        if(epoch % 25 == 24):\n",
    "            print(f\" {epoch+1}\")\n",
    "\n",
    "    print('\\nFinished Training')\n",
    "    PATH = f\"networks/{name}.pth\"\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 36 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenabled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63968e63be298c54f0ef6fc5f307c0f048e4cbeb5634e9b9b715fe9de39ba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
