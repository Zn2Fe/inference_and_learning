{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn.common_types import _size_2_t\n",
    "from torch.nn.modules.utils import _pair\n",
    "from collections import OrderedDict, abc\n",
    "from itertools import repeat\n",
    "\n",
    "from typing import Type, Callable, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and learning final project\n",
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of memory, try an implementation with 32x32 convolutions\n",
    "class LocalLinear_custom(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int,padding : int, image_size: int):\n",
    "        super(LocalLinear_custom, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        fold_num = (image_size+ 2* padding - self.kernel_size)//self.stride+1\n",
    "        self.weight = nn.Parameter(torch.randn( out_channels, in_channels,  fold_num,fold_num, kernel_size, kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels,fold_num,fold_num))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = F.pad(x, [self.padding]*4, value= 0)\n",
    "        x = x.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n",
    "        x = torch.matmul(x.unsqueeze(1),self.weight).squeeze(2).sum([ -5, -1,-2]) + self.bias\n",
    "        return x\n",
    "\n",
    "class view_custom(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funConv2d(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,int,List[nn.Module]]:\n",
    "    padding = kernel_size//2\n",
    "    \n",
    "    modules = [ \n",
    "    nn.Conv2d(size_in,size_out,kernel_size,stride = stride,padding = padding),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()]\n",
    "    im_out = (image_size+ 2 * padding - kernel_size)//stride+1\n",
    "    return im_out,size_out,modules\n",
    "\n",
    "def funLocalLinear(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,int,List[nn.Module]]:\n",
    "    padding = kernel_size//2\n",
    "    modules = [ \n",
    "    LocalLinear_custom(size_in,size_out,kernel_size,stride = stride,padding = padding,image_size = image_size),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()]\n",
    "    im_out = (image_size+ 2* padding - kernel_size)//stride + 1\n",
    "    return im_out,size_out,modules\n",
    "\n",
    "def funFullyConnected(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,int,List[nn.Module]]:\n",
    "    modules = []\n",
    "    modules.extend([\n",
    "    view_custom([-1,image_size*image_size*size_in]),\n",
    "    nn.Linear(size_in * image_size**2,size_out * (image_size//stride)**2),\n",
    "    view_custom([-1,size_out,image_size//stride,image_size//stride]),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()])\n",
    "    im_out = image_size//stride\n",
    "    return im_out,size_out,modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Conv(nn.Module):\n",
    "    ### only works with square image, kernel size and stride\n",
    "    def __init__(self, base_channel_size: int, loc : Callable[[int,int,int,int,int],tuple[int,int,List[nn.Module]]], image_size :int ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        alpha = base_channel_size\n",
    "        channel_size :int = 3\n",
    "        im_size : int = image_size\n",
    "        \n",
    "        modules : List[nn.Module] = []\n",
    "        size_conv = [\n",
    "            (alpha,1),\n",
    "            (2*alpha,2), #remove stride for now\n",
    "            (2*alpha,1),\n",
    "            (4*alpha,2), #here too\n",
    "            (4*alpha,1),\n",
    "            (8*alpha,2), # here too\n",
    "            (8*alpha,1),\n",
    "            (16*alpha,2)] # here too\n",
    "        \n",
    "        for i,val in enumerate(size_conv):\n",
    "            size_out, stride = val\n",
    "            im_size,channel_size,new_module = loc(channel_size,size_out,3,stride,im_size)\n",
    "            modules.extend(new_module)\n",
    "            \n",
    "        self.conv = nn.Sequential(*modules)\n",
    "        im_size,channel_size,modules = funFullyConnected(channel_size,64*alpha,0,1,im_size)\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        self.fc_final = nn.Linear(channel_size*im_size**2,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_final(x)\n",
    "        return x\n",
    "\n",
    "class S_Conv(nn.Module):\n",
    "    def __init__(self, base_channel_size: int, loc : Callable[[int,int,int,int,int],tuple[int,int,List[nn.Module]]], image_size :int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        alpha = base_channel_size\n",
    "        channel_size :int = 3\n",
    "        im_size : int = image_size\n",
    "        \n",
    "        im_size,channel_size, modules = loc(channel_size,alpha,9,2,im_size)\n",
    "        self.conv =  nn.Sequential(*modules)\n",
    "        \n",
    "        im_size,channel_size, modules = funFullyConnected(channel_size,24*alpha,0,1,im_size)\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc_final = nn.Linear(channel_size*im_size**2,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 512\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epo = 2\n",
    "networks = [(D_Conv,\"D\"),(S_Conv,\"S\")]\n",
    "convolutions = [(funConv2d,\"CV\",True),(funLocalLinear,\"LL\",False),(funFullyConnected,\"FC\",True)]\n",
    "nets = [(\"net_\" +str(batch_size) +\"_\"+str(epo)+\"_\"+ni+\"_\"+nj,i,j,dev) for i,ni in networks for j,nj,dev in convolutions ]\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net_128_2_D_CV\n",
      "**\n",
      "Finished Training\n",
      "net_128_2_D_LL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name,net_type,call,dev in nets:\n",
    "    net = net_type(10,call,32).to(DEVICE if dev else \"cpu\" )\n",
    "    print(name)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(epo):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(DEVICE if dev else \"cpu\" ), data[1].to(DEVICE if dev else \"cpu\" )\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print('*', end='')\n",
    "        if(epoch % 25 == 24):\n",
    "            print(f\" {epoch+1}\")\n",
    "\n",
    "    print('\\nFinished Training')\n",
    "    PATH = f\"networks/{name}.pth\"\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    del net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenabled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63968e63be298c54f0ef6fc5f307c0f048e4cbeb5634e9b9b715fe9de39ba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
