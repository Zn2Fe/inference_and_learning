{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To rerun any S-LOCAL network you need a minimum of 32GB of RAM/VRAM memory on the device used for training\n",
    "- Average time to run a 100 epoch on a Nvidia Tesla V100 GPU is 20 minutes\n",
    "- A faster implementation of the Locally Connected Layer is available in the Keras API : \n",
    "- https://keras.io/api/layers/locally_connected_layers/locall_connected2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_PATH = \".\"\n",
    "\n",
    "\n",
    "# To use google colab processing uncomment following lines\n",
    "\n",
    "#INIT_PATH = \"MyDrive/inference_and_learning\"\n",
    "#from google.colab import drive\n",
    "#MOUNT_PATH = \"/content/drive\"\n",
    "#drive.mount(MOUNT_PATH)\n",
    "#INIT_PATH = MOUNT_PATH + \"/\" + INIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(INIT_PATH)\n",
    "import os \n",
    "from os.path import exists\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from networks import NetworkTrainer,NetworkOptimizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".\"\n",
    "if (False):\n",
    "    import torchvision\n",
    "    import torch\n",
    "    torch.set_default_dtype(torch.float16)\n",
    "    torchvision.datasets.CIFAR10(root=DATA_PATH + '/data', download=True)\n",
    "    torchvision.datasets.CIFAR100(root=DATA_PATH + '/data', download=True)\n",
    "    torchvision.datasets.SVHN(root=DATA_PATH +'/data', download=True)\n",
    "\n",
    "    torchvision.datasets.CIFAR10(root=DATA_PATH +'/data', download=True,train = False)\n",
    "    torchvision.datasets.CIFAR100(root=DATA_PATH +'/data', download=True ,train = False) \n",
    "    torchvision.datasets.SVHN(root=DATA_PATH +'/data', download=True, split = \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting S-LOCAL with SGD on CIFAR-10\n",
      "Calculating base accuracy...\n",
      "Training Network: S-LOCAL, CIFAR-10, SGD using : 268.495426 M parameters on cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.10 GiB (GPU 0; 7.78 GiB total capacity; 520.18 MiB already allocated; 6.31 GiB free; 536.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m network_data \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mloc[ID \u001b[39m==\u001b[39m network[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_dict()\n\u001b[1;32m     12\u001b[0m \u001b[39m#if ID[0] == \"S-LOCAL\":\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m#continue\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m best,changed \u001b[39m=\u001b[39m NetworkOptimizer(network_data,\u001b[39mTrue\u001b[39;49;00m,\u001b[39mTrue\u001b[39;49;00m,DATA_PATH)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound best Accuracy : \u001b[39m\u001b[39m{\u001b[39;00mbest[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m % for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(ID)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m changed :    \n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/__init__.py:80\u001b[0m, in \u001b[0;36mNetworkOptimizer\u001b[0;34m(pd_dict, verbose, very_verbose, DATA_PATH)\u001b[0m\n\u001b[1;32m     77\u001b[0m changed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m verbose : \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCalculating base accuracy...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m best[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],_ \u001b[39m=\u001b[39m  NetworkTrainer(best,verbose \u001b[39m=\u001b[39;49m very_verbose,very_verbose \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,DATA_PATH\u001b[39m=\u001b[39;49mDATA_PATH)\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m verbose : \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot accuracy : \u001b[39m\u001b[39m{\u001b[39;00mbest[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m for base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m key,value \u001b[39min\u001b[39;00m optimize\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/__init__.py:50\u001b[0m, in \u001b[0;36mNetworkTrainer\u001b[0;34m(pd_dict, verbose, very_verbose, DATA_PATH, saveTo)\u001b[0m\n\u001b[1;32m     48\u001b[0m inputs,labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(DEVICE),labels\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     49\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     51\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs,labels)\n\u001b[1;32m     52\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/custom_layers.py:224\u001b[0m, in \u001b[0;36mS_Conv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x:torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 224\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_like(x)\n\u001b[1;32m    225\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x,\u001b[39m1\u001b[39m)\n\u001b[1;32m    226\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFC(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/custom_layers.py:103\u001b[0m, in \u001b[0;36mLL_custom.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 103\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal(x)\n\u001b[1;32m    104\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch(x)\n\u001b[1;32m    105\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/custom_layers.py:58\u001b[0m, in \u001b[0;36mLocalLinear_custom.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39munfold(\u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride)\u001b[39m.\u001b[39munfold(\u001b[39m3\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride)\n\u001b[1;32m     57\u001b[0m x\u001b[39m.\u001b[39munsqueeze_(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m x  \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(x,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m])\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n\u001b[1;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.10 GiB (GPU 0; 7.78 GiB total capacity; 520.18 MiB already allocated; 6.31 GiB free; 536.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "table2 = pd.read_csv(INIT_PATH+\"/csv/table2.csv\")\n",
    "network = pd.read_json(INIT_PATH+\"/network_description/table2.json\")\n",
    "for dataset in [\"CIFAR-10\",\"SVHN\",\"CIFAR-100\" ] : #\n",
    "  for line in range(table2.__len__()):\n",
    "        if(table2.loc[line,dataset] == \"NaN\"):\n",
    "            continue\n",
    "        netline = table2.iloc[line]\n",
    "        print(f\"Starting {netline.Model} with {netline.Training_Method} on {dataset}\")\n",
    "        ID= \"_\".join([netline.Model,netline.Training_Method,dataset])\n",
    "        network_data = network.loc[ID == network[\"name\"]].iloc[0].to_dict()\n",
    "        \n",
    "        #if ID[0] == \"S-LOCAL\":\n",
    "            #continue\n",
    "        best,changed = NetworkOptimizer(network_data,True,True,DATA_PATH)\n",
    "        print(f\"Found best Accuracy : {best['accuracy']} % for {'_'.join(ID)}\\n\")\n",
    "        if changed :    \n",
    "            os.remove(INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\")\n",
    "            network.loc[\n",
    "                (network[\"model\"]== netline.Model) &\n",
    "                (network[\"optimizer\"]==netline.Training_Method) &\n",
    "                (network[\"dataset\"]==dataset)].iloc[0] = best\n",
    "            network.to_json(INIT_PATH+\"/network_description/table2.json\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.read_csv(INIT_PATH+\"/csv/table2.csv\")\n",
    "network = pd.read_json(INIT_PATH+\"/network_description/table2.json\")\n",
    "optimize = True\n",
    "for dataset in [\"CIFAR-10\",\"SVHN\",\"CIFAR-100\" ] : #\n",
    "  for line in range(table2.__len__()):\n",
    "        if(table2.loc[line,dataset] == \"NaN\"):\n",
    "            continue\n",
    "        netline = table2.iloc[line]\n",
    "        network_data = network.loc[\n",
    "            (network[\"model\"]== netline.Model) &\n",
    "            (network[\"optimizer\"]==netline.Training_Method) &\n",
    "            (network[\"dataset\"]==dataset)].iloc[0].to_dict()\n",
    "        ID = [network_data[i] for i in [\"model\",\"optimizer\",\"dataset\"]]\n",
    "        # optimize\n",
    "        if optimize : \n",
    "          if ID[0] == \"S-LOCAL\":\n",
    "            continue\n",
    "          best,changed = NetworkOptimizer(network_data,True,True,DATA_PATH)\n",
    "          print(f\"Found best Accuracy : {best['accuracy']} % for {'_'.join(ID)}\\n\")\n",
    "          if changed : \n",
    "            os.remove(INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\")\n",
    "            network.loc[\n",
    "              (network[\"model\"]== netline.Model) &\n",
    "              (network[\"optimizer\"]==netline.Training_Method) &\n",
    "              (network[\"dataset\"]==dataset)].iloc[0] = best\n",
    "        \n",
    "        if not os.path.exists(INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\"):\n",
    "          if ID[0] == \"S-LOCAL\":\n",
    "            continue\n",
    "          PATH = INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\"\n",
    "          acc,_ = NetworkTrainer(network_data,True,False,DATA_PATH,PATH)\n",
    "          table2.loc[line,dataset] = str(acc) + \"%\"\n",
    "          table2.to_csv(\"csv/table2.csv\",index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenabled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63968e63be298c54f0ef6fc5f307c0f048e4cbeb5634e9b9b715fe9de39ba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
