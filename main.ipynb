{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To rerun any S-LOCAL network you need a minimum of 32GB of RAM/VRAM memory on the device used for training\n",
    "- Average time to run a 100 epoch on a Nvidia Tesla V100 GPU is 20 minutes\n",
    "- A faster implementation of the Locally Connected Layer is available in the Keras API : \n",
    "- https://keras.io/api/layers/locally_connected_layers/locall_connected2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_PATH = \".\"\n",
    "\n",
    "\n",
    "# To use google colab processing uncomment following lines\n",
    "\n",
    "#INIT_PATH = \"MyDrive/inference_and_learning\"\n",
    "#from google.colab import drive\n",
    "#MOUNT_PATH = \"/content/drive\"\n",
    "#drive.mount(MOUNT_PATH)\n",
    "#INIT_PATH = MOUNT_PATH + \"/\" + INIT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(INIT_PATH)\n",
    "import os \n",
    "from os.path import exists\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from networks import NetworkTrainer,NetworkOptimizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".\"\n",
    "if (False):\n",
    "    import torchvision\n",
    "    import torch\n",
    "    torch.set_default_dtype(torch.float16)\n",
    "    torchvision.datasets.CIFAR10(root=DATA_PATH + '/data', download=True)\n",
    "    torchvision.datasets.CIFAR100(root=DATA_PATH + '/data', download=True)\n",
    "    torchvision.datasets.SVHN(root=DATA_PATH +'/data', download=True)\n",
    "\n",
    "    torchvision.datasets.CIFAR10(root=DATA_PATH +'/data', download=True,train = False)\n",
    "    torchvision.datasets.CIFAR100(root=DATA_PATH +'/data', download=True ,train = False) \n",
    "    torchvision.datasets.SVHN(root=DATA_PATH +'/data', download=True, split = \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting S-FC with B-lasso(B=0) on CIFAR-10\n",
      "Calculating base accuracy...\n",
      "Training Network: S-FC, CIFAR-10, B-lasso using : 230.60269 M parameters on cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 7.78 GiB total capacity; 1.79 GiB already allocated; 5.25 MiB free; 1.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m network_data \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mloc[ID \u001b[39m==\u001b[39m network[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto_dict()\n\u001b[1;32m     12\u001b[0m \u001b[39m#if ID[0] == \"S-LOCAL\":\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m#continue\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m best,changed \u001b[39m=\u001b[39m NetworkOptimizer(network_data,\u001b[39mTrue\u001b[39;49;00m,\u001b[39mTrue\u001b[39;49;00m,DATA_PATH)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFound best Accuracy : \u001b[39m\u001b[39m{\u001b[39;00mbest[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m % for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(ID)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m changed :    \n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/__init__.py:80\u001b[0m, in \u001b[0;36mNetworkOptimizer\u001b[0;34m(pd_dict, verbose, very_verbose, DATA_PATH)\u001b[0m\n\u001b[1;32m     77\u001b[0m changed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m verbose : \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCalculating base accuracy...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m best[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m],_ \u001b[39m=\u001b[39m  NetworkTrainer(best,verbose \u001b[39m=\u001b[39;49m very_verbose,very_verbose \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,DATA_PATH\u001b[39m=\u001b[39;49mDATA_PATH)\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m verbose : \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot accuracy : \u001b[39m\u001b[39m{\u001b[39;00mbest[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m for base\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m key,value \u001b[39min\u001b[39;00m optimize\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/__init__.py:53\u001b[0m, in \u001b[0;36mNetworkTrainer\u001b[0;34m(pd_dict, verbose, very_verbose, DATA_PATH, saveTo)\u001b[0m\n\u001b[1;32m     51\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs,labels)\n\u001b[1;32m     52\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 53\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     54\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cudaenabled/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/optimizers.py:64\u001b[0m, in \u001b[0;36mB_lasso.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     62\u001b[0m     params : List[Tensor] \u001b[39m=\u001b[39m params_with_grad\n\u001b[1;32m     63\u001b[0m     d_p_list : List[Tensor] \u001b[39m=\u001b[39m d_p_list\n\u001b[0;32m---> 64\u001b[0m     _single_tensor_B_lasso(params,d_p_list,group[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],group[\u001b[39m'\u001b[39;49m\u001b[39ml1_coeff\u001b[39;49m\u001b[39m'\u001b[39;49m],group[\u001b[39m'\u001b[39;49m\u001b[39mBB_l1\u001b[39;49m\u001b[39m'\u001b[39;49m],has_sparse_grad)\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/inference_and_learning/inference_and_learning_final_project/networks/optimizers.py:81\u001b[0m, in \u001b[0;36m_single_tensor_B_lasso\u001b[0;34m(params, grads, lr, l1_coeff, BB_l1, has_sparse_grad)\u001b[0m\n\u001b[1;32m     79\u001b[0m     z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(temps)\n\u001b[1;32m     80\u001b[0m     o \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(temps)\n\u001b[0;32m---> 81\u001b[0m     temps \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(temps \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m, z, o)\n\u001b[1;32m     82\u001b[0m \u001b[39mprint\u001b[39m(temp)\n\u001b[1;32m     83\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(params,temp)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 210.00 MiB (GPU 0; 7.78 GiB total capacity; 1.79 GiB already allocated; 5.25 MiB free; 1.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "table2 = pd.read_csv(INIT_PATH+\"/csv/table2.csv\")\n",
    "network = pd.read_json(INIT_PATH+\"/network_description/table2.json\")\n",
    "for dataset in [\"CIFAR-10\",\"SVHN\",\"CIFAR-100\" ] : #\n",
    "  for line in range(table2.__len__()):\n",
    "        if(table2.loc[line,dataset] == \"NaN\"):\n",
    "            continue\n",
    "        netline = table2.iloc[line]\n",
    "        print(f\"Starting {netline.Model} with {netline.Training_Method} on {dataset}\")\n",
    "        ID= \"_\".join([netline.Model,netline.Training_Method,dataset])\n",
    "        network_data = network.loc[ID == network[\"name\"]].iloc[0].to_dict()\n",
    "        \n",
    "        #if ID[0] == \"S-LOCAL\":\n",
    "            #continue\n",
    "        best,changed = NetworkOptimizer(network_data,True,True,DATA_PATH)\n",
    "        print(f\"Found best Accuracy : {best['accuracy']} % for {'_'.join(ID)}\\n\")\n",
    "        if changed :    \n",
    "            os.remove(INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\")\n",
    "            network.loc[\n",
    "                (network[\"model\"]== netline.Model) &\n",
    "                (network[\"optimizer\"]==netline.Training_Method) &\n",
    "                (network[\"dataset\"]==dataset)].iloc[0] = best\n",
    "            network.to_json(INIT_PATH+\"/network_description/table2.json\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.read_csv(INIT_PATH+\"/csv/table2.csv\")\n",
    "network = pd.read_json(INIT_PATH+\"/network_description/table2.json\")\n",
    "optimize = True\n",
    "for dataset in [\"CIFAR-10\",\"SVHN\",\"CIFAR-100\" ] : #\n",
    "  for line in range(table2.__len__()):\n",
    "        if(table2.loc[line,dataset] == \"NaN\"):\n",
    "            continue\n",
    "        netline = table2.iloc[line]\n",
    "        network_data = network.loc[\n",
    "            (network[\"model\"]== netline.Model) &\n",
    "            (network[\"optimizer\"]==netline.Training_Method) &\n",
    "            (network[\"dataset\"]==dataset)].iloc[0].to_dict()\n",
    "        ID = [network_data[i] for i in [\"model\",\"optimizer\",\"dataset\"]]\n",
    "        # optimize\n",
    "        if optimize : \n",
    "          if ID[0] == \"S-LOCAL\":\n",
    "            continue\n",
    "          best,changed = NetworkOptimizer(network_data,True,True,DATA_PATH)\n",
    "          print(f\"Found best Accuracy : {best['accuracy']} % for {'_'.join(ID)}\\n\")\n",
    "          if changed : \n",
    "            os.remove(INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\")\n",
    "            network.loc[\n",
    "              (network[\"model\"]== netline.Model) &\n",
    "              (network[\"optimizer\"]==netline.Training_Method) &\n",
    "              (network[\"dataset\"]==dataset)].iloc[0] = best\n",
    "        \n",
    "        if not os.path.exists(INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\"):\n",
    "          if ID[0] == \"S-LOCAL\":\n",
    "            continue\n",
    "          PATH = INIT_PATH + f\"/network_saved/table_2_{'_'.join(ID)}.pth\"\n",
    "          acc,_ = NetworkTrainer(network_data,True,False,DATA_PATH,PATH)\n",
    "          table2.loc[line,dataset] = str(acc) + \"%\"\n",
    "          table2.to_csv(\"csv/table2.csv\",index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenabled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63968e63be298c54f0ef6fc5f307c0f048e4cbeb5634e9b9b715fe9de39ba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
