{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    " \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.nn.common_types import _size_2_t\n",
    "from torch.nn.modules.utils import _pair\n",
    "from collections import OrderedDict, abc\n",
    "from itertools import repeat\n",
    "\n",
    "from typing import Type, Callable, List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and learning final project\n",
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Conv(nn.Module):\n",
    "    ### only works with square image, kernel size and stride\n",
    "    def __init__(self, base_channel_size: int, loc : Callable[[int,int,int,int,int],tuple[int,List[nn.Module]]], image_size :int ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        alpha = base_channel_size\n",
    "        channel_size :int = 3\n",
    "        im_size : int = image_size\n",
    "        \n",
    "        modules : List[nn.Module] = []\n",
    "        size_conv = [\n",
    "            (alpha,1),\n",
    "            (2*alpha,1), #remove stride for now\n",
    "            (2*alpha,1),\n",
    "            (4*alpha,1), #here too\n",
    "            (4*alpha,1),\n",
    "            (8*alpha,1), # here too\n",
    "            (8*alpha,1),\n",
    "            (16*alpha,1)] # here too\n",
    "        \n",
    "        for i,val in enumerate(size_conv):\n",
    "            size_out, stride = val\n",
    "            im_size,new_module = loc(channel_size,size_out,3,stride,im_size)\n",
    "            modules.extend(new_module)\n",
    "            channel_size = size_out\n",
    "            \n",
    "        self.conv = nn.Sequential(*modules)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel_size*im_size**2, 64*alpha),\n",
    "            #nn.BatchNorm1d(64*alpha), no this either ?\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64*alpha, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class S_Conv(nn.Module):\n",
    "    def __init__(self, base_channel_size: int, loc : Callable[[int,int,int,int,int],tuple[int,List[nn.Module]]], image_size :int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        alpha = base_channel_size\n",
    "        channel_size :int = 3\n",
    "        im_size : int = image_size\n",
    "        \n",
    "        im_size, modules = loc(channel_size,alpha,9,2,im_size)\n",
    "        channel_size = alpha\n",
    "        \n",
    "        self.conv =  nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel_size*im_size**2, 24*alpha),\n",
    "            #nn.BatchNorm2d(24*alpha), Here too ?\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24*alpha, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of memory \n",
    "class LocalLinear_custom(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_2_t, stride: _size_2_t, image_size: _size_2_t):\n",
    "        super(LocalLinear_custom, self).__init__()\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        image_size = _pair(image_size)\n",
    "        self.stride = _pair(stride)\n",
    "        self.output_size = tuple((image_size[i]-self.kernel_size[i])//self.stride[i]+1 for i in range(2))\n",
    "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, *self.output_size, *self.kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(out_channels,*self.output_size))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x.unfold(2, self.kernel_size[0], self.stride[0]).unfold(3, self.kernel_size[1], self.stride[1])\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        x = torch.matmul(x,self.weight)\n",
    "        x = x.sum([-1,-2,-5]) + self.bias\n",
    "        return x\n",
    "\n",
    "class flatten_custom(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funConv2d(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,List[nn.Module]]:\n",
    "    modules = [ \n",
    "    nn.Conv2d(size_in,size_out,kernel_size,stride = stride),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()]\n",
    "    im_out = (image_size-kernel_size)//stride+1\n",
    "    return im_out,modules\n",
    "\n",
    "def funLocalLinear(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,List[nn.Module]]:\n",
    "    modules = [ \n",
    "    LocalLinear_custom(size_in,size_out,kernel_size,stride,image_size),\n",
    "    nn.BatchNorm2d(size_out),\n",
    "    nn.ReLU()]\n",
    "    im_out = (image_size-kernel_size)//stride+1\n",
    "    return im_out,modules\n",
    "\n",
    "def funFullyConnected(size_in: int,size_out: int, kernel_size: int, stride: int, image_size: int ) -> tuple[int,List[nn.Module]]:\n",
    "    modules = []\n",
    "    if(image_size != 1):\n",
    "        modules.append(flatten_custom())\n",
    "    modules.extend([ \n",
    "    nn.Linear(size_in*image_size**2,size_out),\n",
    "    nn.ReLU()])\n",
    "    im_out = 1\n",
    "    return im_out,modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "networks = [(D_Conv,\"D_Conv\"),(S_Conv,\"S_Conv\")]\n",
    "convolutions = [(funConv2d,\"Conv2d\"),(funFullyConnected,\"FullyConnected\")] #(funLocalLinear,\"LocalLinear\"),\n",
    "\n",
    "nets = [(ni+\"_\"+nj,i,j) for i,ni in networks for j,nj in convolutions ]\n",
    "\n",
    "#net = D_Conv(10,funFullyConnected,32).to(DEVICE)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_Conv_Conv2d\n",
      "************************* 25\n",
      "************************* 50\n",
      "************************* 75\n",
      "************************* 100\n",
      "************************* 125\n",
      "************************* 150\n",
      "************************* 175\n",
      "************************* 200\n",
      "************************* 225\n",
      "************************* 250\n",
      "************************* 275\n",
      "************************* 300\n",
      "************************* 325\n",
      "************************* 350\n",
      "************************* 375\n",
      "************************* 400\n",
      "\n",
      "Finished Training\n",
      "D_Conv_FullyConnected\n",
      "************************* 25\n",
      "************************* 50\n",
      "************************* 75\n",
      "************************* 100\n",
      "************************* 125\n",
      "************************* 150\n",
      "************************* 175\n",
      "************************* 200\n",
      "************************* 225\n",
      "************************* 250\n",
      "************************* 275\n",
      "************************* 300\n",
      "************************* 325\n",
      "************************* 350\n",
      "************************* 375\n",
      "************************* 400\n",
      "\n",
      "Finished Training\n",
      "S_Conv_Conv2d\n",
      "************************* 25\n",
      "************************* 50\n",
      "************************* 75\n",
      "************************"
     ]
    }
   ],
   "source": [
    "for name,net_type,call in nets:\n",
    "    net = net_type(10,call,32).to(DEVICE)\n",
    "    print(name)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(400):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print('*', end='')\n",
    "        if(epoch % 25 == 24):\n",
    "            print(f\" {epoch+1}\")\n",
    "\n",
    "    print('\\nFinished Training')\n",
    "    PATH = f\"networks/{name}.pth\"\n",
    "    torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 36 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(DEVICE), data[1].to(DEVICE)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenabled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63968e63be298c54f0ef6fc5f307c0f048e4cbeb5634e9b9b715fe9de39ba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
