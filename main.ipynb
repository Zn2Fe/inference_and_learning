{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To rerun any S-LOCAL network you need a minimum of 32GB of RAM/VRAM memory on the device used for training\n",
    "- Average time to run a 100 epoch on a Nvidia Tesla V100 GPU is 20 minutes\n",
    "- A faster implementation of the Locally Connected Layer is available in the Keras API : \n",
    "- https://keras.io/api/layers/locally_connected_layers/locall_connected2d/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_PATH = \".\"\n",
    "DOWNLOAD_DATA = False\n",
    "\n",
    "# To use google colab processing uncomment following lines\n",
    "\n",
    "#INIT_PATH = \"MyDrive/inference_and_learning\"\n",
    "#from google.colab import drive\n",
    "#MOUNT_PATH = \"/content/drive\"\n",
    "#drive.mount(MOUNT_PATH)\n",
    "#INIT_PATH = MOUNT_PATH + \"/\" + INIT_PATH\n",
    "#DOWNLOAD_DATA = True\n",
    "\n",
    "# To train LOCAL change to True\n",
    "LOCAL_TRAIN = False \n",
    "\n",
    "\n",
    "# To avoid out of memory error, adjust \n",
    "NUM_THREADS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path,sys\n",
    "sys.path.append(INIT_PATH)\n",
    "\n",
    "import networks as nnets\n",
    "import importlib\n",
    "importlib.reload(nnets)\n",
    "import pandas as pd, torch\n",
    "from typing import Tuple, List\n",
    "import torch, torchvision\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "torch.set_default_dtype(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \".\" # Path to data folder\n",
    "if (DOWNLOAD_DATA):\n",
    "    torchvision.datasets.CIFAR10(root=DATA_PATH + '/data', download=True)\n",
    "    torchvision.datasets.CIFAR100(root=DATA_PATH + '/data', download=True)\n",
    "    torchvision.datasets.SVHN(root=DATA_PATH +'/data', download=True)\n",
    "\n",
    "    torchvision.datasets.CIFAR10(root=DATA_PATH +'/data', download=True,train = False)\n",
    "    torchvision.datasets.CIFAR100(root=DATA_PATH +'/data', download=True ,train = False) \n",
    "    torchvision.datasets.SVHN(root=DATA_PATH +'/data', download=True, split = \"test\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The following code trains network and save result in files\n",
    "\n",
    "Two steps are realized :\n",
    "1. Optimization of the hyperparameters from the optim file\n",
    "2. Training of the network from the train file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tasks remaining\n",
      "\n",
      "\n",
      "Accuracy updated every 100 epochs\n",
      "GPU available: 7.43GB out of 8.35GB\n",
      "GPU max usage: 0.00GB out of 8.35GB\n",
      "\n",
      "\n",
      "     Model     |   Optimizer   |    Dataset    ||    optimization_parameters   ||    accuracy   \n",
      "---------------|---------------|---------------||------------------------------||---------------\n",
      "\n",
      "\n",
      "Thread    ||  Trainer-0  |  Trainer-1  |  Trainer-2  ||\n",
      "----------||-------------|-------------|-------------||\n",
      "Model     || NaN         | NaN         | NaN         ||\n",
      "Dataset   || NaN         | NaN         | NaN         ||\n",
      "Optimizer || NaN         | NaN         | NaN         ||\n",
      "cuda      || cpu         | cpu         | cpu         ||\n",
      "epoch max || 0           | 0           | 0           ||\n",
      "Size      || 0           | 0           | 0           ||\n",
      "----------||-------------|-------------|-------------||\n",
      "Accuracy  || 0:       0% | 0:       0% | 0:       0% ||\n",
      "Epoch     || 0           | 0           | 0           ||\r"
     ]
    }
   ],
   "source": [
    "trainer = nnets.ThreadedTrainer(\n",
    "    num_threads= NUM_THREADS,\n",
    "    DATA_PATH=DATA_PATH,\n",
    "    SAVE_RESULT_PATH= INIT_PATH + \"/files/results.json\",\n",
    "    SAVE_NETWORK_PATH=INIT_PATH + \"/networks_saved/\",\n",
    "    SAVE_NON_ZERO = INIT_PATH + \"/files/non_zero.json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use this script to test different hyperparameters\n",
    "Load the network description in the file files/optim.json\n",
    "The file is a json file with the following structure :\n",
    "```json\n",
    "{\n",
    "    \"optim\":{ // Hyperparameters to optimize\n",
    "        \"model\":{ // Hyperparameters to optimize for the model\n",
    "            \"S-LOCAL\":{ // Model you want to optimize hyperparameters on \n",
    "                \"hidden_size\":[64,128,256,512], // List of values to test\n",
    "                \"dropout\":[0.1,0.2,0.3,0.4,0.5] // List of values to test\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"networks\":{ // List of networks to test\n",
    "        \"S-LOCAL-64-0.1\":{ // Name of the network\n",
    "            //add the network default description here\n",
    "        }\n",
    "    },\n",
    "    \"results\":{ // Results of the optimization\n",
    "        \"S-LOCAL-64-0.1\":{ //Name of the network\n",
    "            \"best\":{ \n",
    "                // Best hyperparameters found\n",
    "            },\n",
    "            \"accuracy\":0.9 // Accuracy of the best hyperparameters\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optim = json.load(open(INIT_PATH+\"/files/optim.json\"))\n",
    "\n",
    "list_to_do = [key for key,item in optim[\"networks\"].items() if\n",
    "              key not in optim[\"results\"] and\n",
    "              not (not LOCAL_TRAIN and item[\"model\"][\"name\"] in [\"S-LOCAL\",\"D-LOCAL\"])]\n",
    "\n",
    "for key in list_to_do:\n",
    "    nnets.network_thread(optim,key, trainer)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may use this script to train networks from the /files/train.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING !!! :\n",
      "Trainer-2 error : [Errno 2] No such file or directory: './networks_saved/S-CONV_SGD_CIFAR-100_INCOMPLETE.pth'\n",
      "Trainer-0 error : [Errno 2] No such file or directory: './networks_saved/S-CONV_SGD_SVHN_INCOMPLETE.pth'\n",
      "\n",
      "\n",
      "15 tasks remaining\n",
      "\t1. 3-FC_SGD_CIFAR-100                     9. S-FC_B-lasso(B=10)_CIFAR-100\n",
      "\t2. 3-FC_SGD_SVHN                          10. S-FC_B-lasso(B=10)_SVHN\n",
      "\t3. S-FC_SGD_CIFAR-100                     11. S-FC_B-lasso(B=50)_CIFAR-10\n",
      "\t4. S-FC_SGD_SVHN                          12. S-FC_B-lasso(B=50)_CIFAR-100\n",
      "\t5. S-FC_B-lasso(B=0)_CIFAR-10             13. S-FC_B-lasso(B=50)_SVHN\n",
      "\t6. S-FC_B-lasso(B=0)_CIFAR-100            14. S-CONV_SGD_CIFAR-100\n",
      "\t7. S-FC_B-lasso(B=0)_SVHN                 15. S-CONV_SGD_SVHN\n",
      "\t8. S-FC_B-lasso(B=10)_CIFAR-10\n",
      "\n",
      "\n",
      "Accuracy updated every 100 epochs\n",
      "GPU available: 5.59GB out of 8.35GB\n",
      "GPU max usage: 1.49GB out of 8.35GB\n",
      "\n",
      "\n",
      "     Model     |   Optimizer   |    Dataset    ||    optimization_parameters   ||    accuracy   \n",
      "---------------|---------------|---------------||------------------------------||---------------\n",
      "\n",
      "\n",
      "Thread    ||  Trainer-0  |  Trainer-1  |  Trainer-2  ||\n",
      "----------||-------------|-------------|-------------||\n",
      "Model     || NaN         | 3-FC        | NaN         ||\n",
      "Dataset   || NaN         | CIFAR-10    | NaN         ||\n",
      "Optimizer || NaN         | SGD         | NaN         ||\n",
      "cuda      || cpu         | cuda        | cpu         ||\n",
      "epoch max || 0           | 4000        | 0           ||\n",
      "Size      || 0           | 249.79 M    | 0           ||\n",
      "----------||-------------|-------------|-------------||\n",
      "Accuracy  || 0:       0% | 0:       0% | 0:       0% ||\n",
      "Epoch     || 0           | 11          | 0           ||\r"
     ]
    }
   ],
   "source": [
    "trainer.acc_update = 100\n",
    "RETRAIN = False\n",
    "train_net = json.load(open(INIT_PATH+\"/files/train.json\"))\n",
    "for key,item in train_net.items():\n",
    "    if item[\"model\"][\"name\"] in [\"S-LOCAL\",\"D-LOCAL\"] and not LOCAL_TRAIN:\n",
    "        continue\n",
    "    if not RETRAIN and os.path.isfile(INIT_PATH+\"/networks_saved/\"+key+\".pth\"):\n",
    "        continue\n",
    "\n",
    "    trainer.add(key,item,item[\"model\"][\"name\"] == \"S-FC\" and item[\"optimizer\"][\"name\"] == \"B-lasso(B=50)\")\n",
    "\n",
    "trainer.logger.big_update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the paper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.read_csv(INIT_PATH+\"/files/table2.csv\")\n",
    "results = json.load(open(INIT_PATH+\"/files/results.json\"))\n",
    "\n",
    "for dataset in [\"CIFAR10\",\"CIFAR100\",\"SVHN\"]:\n",
    "    for line in range(len(table2)):\n",
    "        iline = table2.iloc[line].to_dict()\n",
    "        if not iline[\"train\"] : continue\n",
    "        if iline[\"dataset\"] == \"nan\": continue\n",
    "        ID = iline[\"Model\"] + \"_\" + iline[\"Training_Method\"] + \"_\" + dataset\n",
    "        table2.loc[line,dataset] = results[ID]\n",
    "\n",
    "display(HTML(table2.to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### figure 3\n",
    "We can reuse result saved while training table 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenabled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e63968e63be298c54f0ef6fc5f307c0f048e4cbeb5634e9b9b715fe9de39ba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
